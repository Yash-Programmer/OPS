{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209d320f",
   "metadata": {},
   "source": [
    "# Phase 3: Model Training\n",
    "\n",
    "**Objective:** Train 6 ML models on all datasets and implement baseline methods\n",
    "\n",
    "## Timeline: Week 4\n",
    "\n",
    "### Tasks:\n",
    "1. Train models on each dataset:\n",
    "   - Logistic Regression / Linear Regression\n",
    "   - Random Forest\n",
    "   - XGBoost\n",
    "   - Neural Network (MLP)\n",
    "   - SVM\n",
    "   - Decision Tree\n",
    "\n",
    "2. Implement baseline comparison methods:\n",
    "   - KernelSHAP\n",
    "   - TreeExplainer (for tree-based models)\n",
    "\n",
    "3. Save trained models for experiments\n",
    "\n",
    "### Success Criteria:\n",
    "- All models achieve reasonable accuracy\n",
    "- Models saved to `data/models/`\n",
    "- Baselines ready for Phase 4 comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e105de94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "Working directory: c:\\Users\\Yash\\Music\\jisads research\\OPS_Project\n",
      "Models will be saved to: c:\\Users\\Yash\\Music\\jisads research\\OPS_Project\\data\\models\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML models\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# SHAP baselines\n",
    "import shap\n",
    "\n",
    "# Setup paths\n",
    "project_root = Path.cwd() / 'OPS_Project'\n",
    "data_dir = project_root / 'data' / 'processed'\n",
    "models_dir = project_root / 'data' / 'models'\n",
    "models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"Working directory: {project_root}\")\n",
    "print(f\"Models will be saved to: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81d653e",
   "metadata": {},
   "source": [
    "## 1. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "289262f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded iris: X shape (150, 4), y shape (150,)\n",
      "âœ… Loaded california_housing: X shape (20640, 8), y shape (20640,)\n",
      "âœ… Loaded adult_income: X shape (5000, 14), y shape (5000,)\n",
      "âœ… Loaded mnist_pca: X shape (2000, 50), y shape (2000,)\n",
      "âœ… Loaded synthetic_svm: X shape (1000, 100), y shape (1000,)\n",
      "âœ… Loaded non_submodular: X shape (500, 10), y shape (500,)\n",
      "\n",
      "Total datasets loaded: 6\n"
     ]
    }
   ],
   "source": [
    "# Load all 6 datasets\n",
    "datasets = {}\n",
    "\n",
    "dataset_names = [\n",
    "    'iris',\n",
    "    'california_housing',\n",
    "    'adult_income',\n",
    "    'mnist_pca',\n",
    "    'synthetic_svm',\n",
    "    'non_submodular'\n",
    "]\n",
    "\n",
    "for name in dataset_names:\n",
    "    with open(data_dir / f'{name}.pkl', 'rb') as f:\n",
    "        datasets[name] = pickle.load(f)\n",
    "    print(f\"âœ… Loaded {name}: X shape {datasets[name]['X'].shape}, y shape {datasets[name]['y'].shape}\")\n",
    "\n",
    "print(f\"\\nTotal datasets loaded: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba61b4e",
   "metadata": {},
   "source": [
    "## 2. Define Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cba92ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training pipeline defined\n"
     ]
    }
   ],
   "source": [
    "def get_models(task_type='classification'):\n",
    "    \"\"\"\n",
    "    Get 6 ML models for classification or regression.\n",
    "    \n",
    "    Args:\n",
    "        task_type: 'classification' or 'regression'\n",
    "        \n",
    "    Returns:\n",
    "        dict: Model name -> model instance\n",
    "    \"\"\"\n",
    "    if task_type == 'classification':\n",
    "        return {\n",
    "            'logistic': LogisticRegression(max_iter=1000, random_state=42),\n",
    "            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "            'xgboost': xgb.XGBClassifier(n_estimators=100, random_state=42, max_depth=6, eval_metric='logloss'),\n",
    "            'neural_net': MLPClassifier(hidden_layers=(100, 50), max_iter=500, random_state=42),\n",
    "            'svm': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "            'decision_tree': DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "        }\n",
    "    else:  # regression\n",
    "        return {\n",
    "            'linear': LinearRegression(),\n",
    "            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10),\n",
    "            'xgboost': xgb.XGBRegressor(n_estimators=100, random_state=42, max_depth=6),\n",
    "            'neural_net': MLPRegressor(hidden_layers=(100, 50), max_iter=500, random_state=42),\n",
    "            'svm': SVR(kernel='rbf'),\n",
    "            'decision_tree': DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "        }\n",
    "\n",
    "def train_and_evaluate(X, y, model, task_type='classification'):\n",
    "    \"\"\"\n",
    "    Train model and evaluate performance.\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        metrics: Performance metrics\n",
    "        X_train, X_test, y_train, y_test: Train/test splits\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y if task_type == 'classification' else None\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='weighted')\n",
    "        }\n",
    "    else:\n",
    "        metrics = {\n",
    "            'mse': mean_squared_error(y_test, y_pred),\n",
    "            'r2': r2_score(y_test, y_pred)\n",
    "        }\n",
    "    \n",
    "    return model, metrics, X_train, X_test, y_train, y_test\n",
    "\n",
    "print(\"âœ… Training pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb53dbc",
   "metadata": {},
   "source": [
    "## 3. Train Models on All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14acd647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING MODELS ON ALL DATASETS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Dataset: IRIS\n",
      "================================================================================\n",
      "Task type: classification\n",
      "Data shape: X=(150, 4), y=(150,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MLPClassifier.__init__() got an unexpected keyword argument 'hidden_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData shape: X=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, y=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Get models for this task\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mget_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m trained_models[dataset_name] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[12], line 16\u001b[0m, in \u001b[0;36mget_models\u001b[1;34m(task_type)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mGet 6 ML models for classification or regression.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    dict: Model name -> model instance\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic\u001b[39m\u001b[38;5;124m'\u001b[39m: LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_forest\u001b[39m\u001b[38;5;124m'\u001b[39m: RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m'\u001b[39m: xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneural_net\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mMLPClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m'\u001b[39m: SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m, probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecision_tree\u001b[39m\u001b[38;5;124m'\u001b[39m: DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     19\u001b[0m     }\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# regression\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m: LinearRegression(),\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_forest\u001b[39m\u001b[38;5;124m'\u001b[39m: RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecision_tree\u001b[39m\u001b[38;5;124m'\u001b[39m: DecisionTreeRegressor(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     28\u001b[0m     }\n",
      "\u001b[1;31mTypeError\u001b[0m: MLPClassifier.__init__() got an unexpected keyword argument 'hidden_layers'"
     ]
    }
   ],
   "source": [
    "# Configuration: dataset -> task type\n",
    "dataset_config = {\n",
    "    'iris': 'classification',\n",
    "    'california_housing': 'regression',\n",
    "    'adult_income': 'classification',\n",
    "    'mnist_pca': 'classification',\n",
    "    'synthetic_svm': 'classification',\n",
    "    'non_submodular': 'regression'\n",
    "}\n",
    "\n",
    "# Store all trained models and results\n",
    "trained_models = {}\n",
    "results_summary = []\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING MODELS ON ALL DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Dataset: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    X = datasets[dataset_name]['X']\n",
    "    y = datasets[dataset_name]['y']\n",
    "    task_type = dataset_config[dataset_name]\n",
    "    \n",
    "    print(f\"Task type: {task_type}\")\n",
    "    print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    # Get models for this task\n",
    "    models = get_models(task_type)\n",
    "    \n",
    "    trained_models[dataset_name] = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n  Training {model_name}...\", end=' ')\n",
    "        \n",
    "        try:\n",
    "            trained_model, metrics, X_train, X_test, y_train, y_test = train_and_evaluate(\n",
    "                X, y, model, task_type\n",
    "            )\n",
    "            \n",
    "            # Store model and splits\n",
    "            trained_models[dataset_name][model_name] = {\n",
    "                'model': trained_model,\n",
    "                'X_train': X_train,\n",
    "                'X_test': X_test,\n",
    "                'y_train': y_train,\n",
    "                'y_test': y_test,\n",
    "                'metrics': metrics,\n",
    "                'task_type': task_type\n",
    "            }\n",
    "            \n",
    "            # Save model\n",
    "            model_path = models_dir / f'{dataset_name}_{model_name}.pkl'\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(trained_models[dataset_name][model_name], f)\n",
    "            \n",
    "            # Print metrics\n",
    "            if task_type == 'classification':\n",
    "                print(f\"Acc: {metrics['accuracy']:.4f}, F1: {metrics['f1_score']:.4f} âœ…\")\n",
    "            else:\n",
    "                print(f\"MSE: {metrics['mse']:.4f}, RÂ²: {metrics['r2']:.4f} âœ…\")\n",
    "            \n",
    "            # Record summary\n",
    "            results_summary.append({\n",
    "                'dataset': dataset_name,\n",
    "                'model': model_name,\n",
    "                'task': task_type,\n",
    "                **metrics\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"FAILED: {e} âŒ\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… MODEL TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total models trained: {len(results_summary)}\")\n",
    "print(f\"Models saved to: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec32faa",
   "metadata": {},
   "source": [
    "## 4. Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b05b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL PERFORMANCE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "CLASSIFICATION TASKS:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'task'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCLASSIFICATION TASKS:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m clf_results \u001b[38;5;241m=\u001b[39m results_df[\u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(clf_results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(clf_results[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\Yash\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Yash\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'task'"
     ]
    }
   ],
   "source": [
    "# Create summary DataFrame\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Classification results\n",
    "print(\"\\nCLASSIFICATION TASKS:\")\n",
    "print(\"-\" * 80)\n",
    "clf_results = results_df[results_df['task'] == 'classification']\n",
    "if len(clf_results) > 0:\n",
    "    print(clf_results[['dataset', 'model', 'accuracy', 'f1_score']].to_string(index=False))\n",
    "\n",
    "# Regression results\n",
    "print(\"\\n\\nREGRESSION TASKS:\")\n",
    "print(\"-\" * 80)\n",
    "reg_results = results_df[results_df['task'] == 'regression']\n",
    "if len(reg_results) > 0:\n",
    "    print(reg_results[['dataset', 'model', 'mse', 'r2']].to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "results_df.to_csv(project_root / 'results' / 'model_training_summary.csv', index=False)\n",
    "print(f\"\\nâœ… Summary saved to: {project_root / 'results' / 'model_training_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ecb2e",
   "metadata": {},
   "source": [
    "## 5. Implement Baseline Methods (KernelSHAP & TreeExplainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39419bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Baseline methods (KernelSHAP, TreeExplainer) implemented\n"
     ]
    }
   ],
   "source": [
    "class BaselineMethods:\n",
    "    \"\"\"\n",
    "    Wrapper for baseline Shapley estimation methods.\n",
    "    Implements KernelSHAP and TreeExplainer for comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def kernelshap(model, X_background, X_explain, n_samples=1000):\n",
    "        \"\"\"\n",
    "        Estimate Shapley values using KernelSHAP.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained model\n",
    "            X_background: Background dataset for sampling\n",
    "            X_explain: Data points to explain\n",
    "            n_samples: Number of samples for kernel approximation\n",
    "            \n",
    "        Returns:\n",
    "            shap_values: SHAP values (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        # Create KernelExplainer\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            explainer = shap.KernelExplainer(model.predict_proba, X_background)\n",
    "        else:\n",
    "            explainer = shap.KernelExplainer(model.predict, X_background)\n",
    "        \n",
    "        # Compute SHAP values\n",
    "        shap_values = explainer.shap_values(X_explain, nsamples=n_samples)\n",
    "        \n",
    "        return shap_values\n",
    "    \n",
    "    @staticmethod\n",
    "    def tree_explainer(model, X_explain):\n",
    "        \"\"\"\n",
    "        Estimate Shapley values using TreeExplainer (for tree-based models).\n",
    "        \n",
    "        Args:\n",
    "            model: Trained tree-based model\n",
    "            X_explain: Data points to explain\n",
    "            \n",
    "        Returns:\n",
    "            shap_values: Exact SHAP values (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        # Create TreeExplainer\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        \n",
    "        # Compute SHAP values\n",
    "        shap_values = explainer.shap_values(X_explain)\n",
    "        \n",
    "        return shap_values\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_tree_based(model):\n",
    "        \"\"\"\n",
    "        Check if model is tree-based (can use TreeExplainer).\n",
    "        \"\"\"\n",
    "        tree_models = (\n",
    "            RandomForestClassifier, RandomForestRegressor,\n",
    "            DecisionTreeClassifier, DecisionTreeRegressor,\n",
    "            xgb.XGBClassifier, xgb.XGBRegressor\n",
    "        )\n",
    "        return isinstance(model, tree_models)\n",
    "\n",
    "print(\"âœ… Baseline methods (KernelSHAP, TreeExplainer) implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5af3e72",
   "metadata": {},
   "source": [
    "## 6. Test Baseline Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test on Iris dataset with Random Forest\n",
    "print(\"Testing baseline methods on Iris dataset (Random Forest)...\\n\")\n",
    "\n",
    "test_dataset = 'iris'\n",
    "test_model_name = 'random_forest'\n",
    "\n",
    "model_data = trained_models[test_dataset][test_model_name]\n",
    "model = model_data['model']\n",
    "X_test = model_data['X_test'][:5]  # Test on 5 samples\n",
    "X_background = model_data['X_train'][:100]  # Background for KernelSHAP\n",
    "\n",
    "print(f\"Model: {test_model_name}\")\n",
    "print(f\"Test samples: {X_test.shape}\")\n",
    "print(f\"Background samples: {X_background.shape}\")\n",
    "\n",
    "# TreeExplainer (exact for tree models)\n",
    "if BaselineMethods.is_tree_based(model):\n",
    "    print(\"\\n1. TreeExplainer (exact):\")\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    shap_tree = BaselineMethods.tree_explainer(model, X_test)\n",
    "    t_tree = time.time() - t0\n",
    "    \n",
    "    if isinstance(shap_tree, list):  # Multi-class\n",
    "        shap_tree = shap_tree[0]  # Use first class\n",
    "    \n",
    "    print(f\"   Shape: {shap_tree.shape}\")\n",
    "    print(f\"   Time: {t_tree*1000:.2f} ms\")\n",
    "    print(f\"   Sample values (1st instance): {shap_tree[0]}\")\n",
    "\n",
    "# KernelSHAP (approximation)\n",
    "print(\"\\n2. KernelSHAP (n_samples=100):\")\n",
    "t0 = time.time()\n",
    "shap_kernel = BaselineMethods.kernelshap(model, X_background, X_test, n_samples=100)\n",
    "t_kernel = time.time() - t0\n",
    "\n",
    "if isinstance(shap_kernel, list):  # Multi-class\n",
    "    shap_kernel = shap_kernel[0]\n",
    "\n",
    "print(f\"   Shape: {shap_kernel.shape}\")\n",
    "print(f\"   Time: {t_kernel*1000:.2f} ms\")\n",
    "print(f\"   Sample values (1st instance): {shap_kernel[0]}\")\n",
    "\n",
    "print(\"\\nâœ… Baseline methods tested successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd195ec",
   "metadata": {},
   "source": [
    "## 7. Phase 3 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5dcf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PHASE 3 COMPLETE: MODEL TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ… Achievements:\")\n",
    "print(f\"   - Trained {len(results_summary)} models across 6 datasets\")\n",
    "print(f\"   - 6 model types: Logistic/Linear, Random Forest, XGBoost, Neural Net, SVM, Decision Tree\")\n",
    "print(f\"   - All models saved to: {models_dir}\")\n",
    "print(f\"   - Baseline methods implemented: KernelSHAP, TreeExplainer\")\n",
    "print(f\"   - Performance summary saved: {project_root / 'results' / 'model_training_summary.csv'}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Model Performance:\")\n",
    "print(f\"   - Classification tasks: {len(clf_results)} models\")\n",
    "print(f\"     Average accuracy: {clf_results['accuracy'].mean():.4f}\")\n",
    "print(f\"   - Regression tasks: {len(reg_results)} models\")\n",
    "print(f\"     Average RÂ²: {reg_results['r2'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Next: Phase 4 - Experimental Evaluation\")\n",
    "print(\"   Run full experiments: 6 datasets Ã— 6 models Ã— 5 algorithms Ã— 5 budgets\")\n",
    "print(\"   Total experiments: 900+ configurations\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
