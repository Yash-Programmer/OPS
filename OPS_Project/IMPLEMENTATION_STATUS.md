# Orthogonal Permutation Sampling (OPS) for Shapley Values - Implementation

Research implementation of the paper "Orthogonal Permutation Sampling for Shapley Values" achieving 5-67Ã— variance reduction over Monte Carlo methods.

## ğŸ“Š Project Status

**Implementation Progress: 30% Complete (3/10 Phases)**

- âœ… **Phase 1**: Environment & Dataset Preparation
- âœ… **Phase 2**: Core Algorithm Implementation  
- âœ… **Phase 3**: Model Training (36 models trained)
- ğŸ”„ **Phase 4**: Experimental Evaluation (in progress)
- â³ **Phases 5-10**: Pending

## ğŸ¯ Key Achievements

### Phase 1: Datasets
- 6 benchmark datasets generated (150-20,640 samples, 4-100 features)
- Iris (n=4), California Housing (n=8), Adult Income (n=14), MNIST-PCA (n=50), Synthetic-SVM (n=100), Non-submodular (n=10)

### Phase 2: Algorithms Implemented
1. **Monte Carlo Baseline** - Naive permutation sampling
2. **Position-Stratified (PS)** - Algorithm 1 with rank stratification
3. **Neyman Allocation** - Optimal budget allocation (Corollary 1)
4. **Orthogonal Permutation Sampling (OPS)** - Antithetic coupling (Algorithm 2)
5. **OPS with Control Variates (OPS-CV)** - Linearized surrogate (Algorithm 3)

### Phase 3: Models Trained
- **36 models** across 6 datasets Ã— 6 model types
- Model types: Logistic/Linear Regression, Random Forest, XGBoost, Neural Network, SVM, Decision Tree
- **Classification performance**: 83.1% avg accuracy, 83.0% avg F1
- **Regression performance**: 47.5% avg RÂ²

## ğŸ“ Project Structure

```
OPS_Project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ algorithms/          # Core OPS algorithms
â”‚   â”‚   â”œâ”€â”€ shapley_base.py
â”‚   â”‚   â”œâ”€â”€ position_stratified.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ baselines/           # Comparison methods
â”‚       â”œâ”€â”€ shap_baselines.py
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/           # 6 benchmark datasets (.pkl)
â”‚   â””â”€â”€ models/              # 36 trained models (.pkl)
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ experiments/         # Experimental results
â”‚   â””â”€â”€ model_training_summary.csv
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ OPS_Implementation_Phase1.ipynb
â”‚   â”œâ”€â”€ OPS_Implementation_Phase2.ipynb
â”‚   â””â”€â”€ OPS_Implementation_Phase3.ipynb
â”œâ”€â”€ generate_datasets.py     # Dataset generation script
â”œâ”€â”€ train_models.py          # Model training script
â”œâ”€â”€ test_algorithms.py       # Algorithm validation
â””â”€â”€ run_experiments.py       # Experimental evaluation
```

## ğŸš€ Quick Start

### 1. Generate Datasets
```bash
python generate_datasets.py
```

### 2. Train Models
```bash
python train_models.py
```

### 3. Test Algorithms
```bash
python test_algorithms.py
```

### 4. Run Experiments
```bash
python run_experiments.py
```

## ğŸ“ˆ Model Performance Summary

### Classification Tasks (24 models)
| Dataset | Best Model | Accuracy | F1 Score |
|---------|------------|----------|----------|
| Iris | Neural Net | 1.000 | 1.000 |
| Adult Income | Neural Net | 0.964 | 0.964 |
| MNIST-PCA | SVM | 0.828 | 0.828 |
| Synthetic-SVM | SVM | 0.940 | 0.940 |

**Average**: 83.1% accuracy, 83.0% F1 score

### Regression Tasks (12 models)
| Dataset | Best Model | MSE | RÂ² |
|---------|------------|-----|-----|
| California Housing | XGBoost | 0.223 | 0.830 |
| Non-submodular | Neural Net | 0.718 | 0.787 |

**Average**: RÂ² = 0.475

## ğŸ”¬ Algorithms Overview

### 1. Monte Carlo (MC) Baseline
- Naive uniform permutation sampling
- Reference baseline for variance reduction

### 2. Position-Stratified Shapley (PS)
- Stratifies by feature rank k âˆˆ {0, ..., n-1}
- **Theorem 1**: Eliminates between-stratum variance
- Uniform budget allocation: L_k = L/n

### 3. Neyman Allocation
- Optimal allocation proportional to stratum std dev
- **Corollary 1**: L_k* = L Â· Ïƒ_k / Î£(Ïƒ_j)
- Two-phase: pilot + allocation

### 4. Orthogonal Permutation Sampling (OPS)
- Antithetic pairs: Ï€ and Ï€^âŠ¥
- **Theorem 3**: â‰¥2Ã— variance reduction when negatively correlated
- Construction: Ï€^âŠ¥(j) = n - 1 - Ï€(n - 1 - j)

### 5. OPS with Control Variates (OPS-CV)
- Uses linearized surrogate model
- **Theorem 4**: Var[Ï†_CV] = Var[Ï†_OPS](1 - ÏÂ²)
- Additional reduction proportional to correlation Ï

## ğŸ“Š Expected Results (from paper)

- **Variance Reduction**: 5-67Ã— over Monte Carlo
- **MSE Reduction**: 2-5Ã— lower than KernelSHAP
- **Computation**: Comparable cost to naive MC
- **Datasets**: Effective across n=4 to n=100 features

## ğŸ”§ Technical Details

### Dependencies
- Python 3.10+
- NumPy, pandas, scikit-learn
- XGBoost, SHAP
- matplotlib, seaborn (for visualization)

### Experimental Configuration
- **Budgets**: L âˆˆ {100, 500, 1000, 2500, 5000}
- **Trials**: 30-50 per configuration
- **Metrics**: Variance, MSE, computation time
- **Baselines**: KernelSHAP, TreeExplainer

## ğŸ“ Implementation Notes

### Phase 1 Completed
- All datasets generated with proper preprocessing
- Train/test splits with stratification
- Dataset statistics validated

### Phase 2 Completed
- All 5 algorithms implemented and tested
- Modular class hierarchy (inheritance from ShapleyEstimator)
- Exact Shapley computation for nâ‰¤10 validation
- Test script confirms correctness on linear model

### Phase 3 Completed
- 36 models trained successfully
- Models saved with train/test splits
- Performance metrics recorded
- Ready for Shapley value experiments

### Phase 4 In Progress
- Experimental framework created
- Running initial MC vs PS comparison
- 1,800 total experiments planned
- Results saved incrementally

## ğŸ¯ Next Steps

### Phase 4: Complete Experimental Evaluation
- [ ] Finish MC vs PS experiments (6 configs)
- [ ] Add Neyman, OPS, OPS-CV to comparison
- [ ] Expand to all 36 model/dataset pairs
- [ ] Compute variance reduction factors

### Phase 5: Visualization & Analysis
- [ ] Variance vs budget plots
- [ ] VRF heatmaps by dataset/model
- [ ] MSE comparison with baselines
- [ ] Computation time analysis

### Phase 6-10: Advanced Analysis
- [ ] Ablation studies
- [ ] Non-submodular game analysis
- [ ] High-dimensional experiments
- [ ] Optimization & parallelization
- [ ] Final validation & presentation

## ğŸ“š References

**Paper**: "Orthogonal Permutation Sampling for Shapley Values" (Yash Varshney, 2025)

**Key Contributions**:
1. Position stratification with variance decomposition
2. Neyman allocation for optimal sampling
3. Orthogonal permutation coupling (antithetic variance reduction)
4. Control variate acceleration

## ğŸ¤ Contact

Research implementation by: Yash Varshney  
Date: November 2025

---

**Status Last Updated**: November 5, 2025
