{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1740b6",
   "metadata": {},
   "source": [
    "# OPS Implementation - Phase 2: Core Algorithm Implementation\n",
    "\n",
    "**Weeks 2-3: Implementing all OPS variants**\n",
    "\n",
    "This notebook implements the five core algorithms from the paper:\n",
    "1. **ShapleyEstimator** - Naive MC baseline + Exact Shapley\n",
    "2. **PositionStratifiedShapley** - Algorithm 1 (PS)\n",
    "3. **NeymanAllocator** - Corollary 1 (Optimal allocation)\n",
    "4. **OrthogonalPermutationSampling** - Algorithm 2 (OPS with antithetic coupling)\n",
    "5. **OPSWithControlVariate** - Algorithm 3 (OPS-CV)\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 2 Goals:\n",
    "\n",
    "✅ Implement all core algorithms following paper exactly  \n",
    "✅ Validate Theorem 1 (variance decomposition)  \n",
    "✅ Validate Theorem 2 (non-positive covariance)  \n",
    "✅ Unit tests for unbiasedness  \n",
    "✅ Comparison: MC vs PS vs OPS vs OPS-CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad434953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n",
      "Working directory: c:\\Users\\Yash\\Music\\jisads research\\OPS_Project\n",
      "NumPy seed: 42 (for reproducibility)\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Callable, Set, Dict, List, Tuple, Optional\n",
    "from itertools import combinations, permutations\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Project paths\n",
    "project_root = Path(\"c:/Users/Yash/Music/jisads research/OPS_Project\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"Working directory: {project_root}\")\n",
    "print(f\"NumPy seed: 42 (for reproducibility)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222429e8",
   "metadata": {},
   "source": [
    "## Step 2.1: Base Shapley Estimator\n",
    "\n",
    "Implementing naive Monte Carlo baseline and exact Shapley computation for validation.\n",
    "\n",
    "### Theory (from paper):\n",
    "\n",
    "**Definition 2 (Shapley Value - Permutation Form):**\n",
    "\n",
    "$$\\phi_i(v) = \\mathbb{E}_{\\pi \\sim \\text{Unif}(\\Pi_n)} [\\Delta_i v(P_i(\\pi))]$$\n",
    "\n",
    "where:\n",
    "- $\\Pi_n$ = set of all $n!$ permutations\n",
    "- $P_i(\\pi)$ = predecessors of feature $i$ in permutation $\\pi$\n",
    "- $\\Delta_i v(S) = v(S \\cup \\{i\\}) - v(S)$ = marginal contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a51dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ShapleyEstimator class implemented\n",
      "   - exact_shapley(): Enumerate n! permutations (n≤10)\n",
      "   - mc_shapley(): Naive Monte Carlo baseline\n"
     ]
    }
   ],
   "source": [
    "class ShapleyEstimator:\n",
    "    \"\"\"\n",
    "    Base Shapley value estimator implementing:\n",
    "    1. Exact computation via permutation enumeration (for n≤10)\n",
    "    2. Naive Monte Carlo sampling (baseline for comparison)\n",
    "    \n",
    "    References:\n",
    "        - Definition 2 (Shapley Value - Permutation Form) from paper\n",
    "        - Equations 4, 5, 6 from paper\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: Callable, X_instance: np.ndarray, baseline: Optional[np.ndarray] = None):\n",
    "        \"\"\"\n",
    "        Initialize Shapley estimator.\n",
    "        \n",
    "        Args:\n",
    "            model: Prediction function (callable)\n",
    "            X_instance: Single instance to explain (1D array)\n",
    "            baseline: Baseline/reference instance for masking (default: zeros)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.X_instance = X_instance\n",
    "        self.n_features = len(X_instance)\n",
    "        self.baseline = baseline if baseline is not None else np.zeros_like(X_instance)\n",
    "        \n",
    "        # Feature indices\n",
    "        self.N = set(range(self.n_features))\n",
    "        \n",
    "    def _evaluate_coalition(self, S: Set[int]) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate model on coalition S.\n",
    "        \n",
    "        Args:\n",
    "            S: Coalition (set of feature indices)\n",
    "            \n",
    "        Returns:\n",
    "            Model prediction with features in S from X_instance, others from baseline\n",
    "        \"\"\"\n",
    "        X_masked = self.baseline.copy()\n",
    "        for i in S:\n",
    "            X_masked[i] = self.X_instance[i]\n",
    "        \n",
    "        # Handle different model types\n",
    "        try:\n",
    "            return float(self.model(X_masked.reshape(1, -1))[0])\n",
    "        except:\n",
    "            return float(self.model(X_masked.reshape(1, -1)))\n",
    "    \n",
    "    def marginal_contribution(self, feature_idx: int, S: Set[int]) -> float:\n",
    "        \"\"\"\n",
    "        Compute marginal contribution Δᵢv(S) = v(S ∪ {i}) - v(S).\n",
    "        \n",
    "        Args:\n",
    "            feature_idx: Feature index i\n",
    "            S: Coalition not containing i\n",
    "            \n",
    "        Returns:\n",
    "            Marginal contribution\n",
    "        \"\"\"\n",
    "        assert feature_idx not in S, \"Feature must not be in coalition\"\n",
    "        \n",
    "        S_with_i = S | {feature_idx}\n",
    "        return self._evaluate_coalition(S_with_i) - self._evaluate_coalition(S)\n",
    "    \n",
    "    def exact_shapley(self, feature_idx: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute exact Shapley value by enumerating all n! permutations.\n",
    "        Only feasible for n≤10.\n",
    "        \n",
    "        Args:\n",
    "            feature_idx: Feature index\n",
    "            \n",
    "        Returns:\n",
    "            Exact Shapley value\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If n > 10 (computational infeasibility)\n",
    "        \"\"\"\n",
    "        if self.n_features > 10:\n",
    "            raise ValueError(f\"Exact computation infeasible for n={self.n_features} > 10\")\n",
    "        \n",
    "        N_minus_i = self.N - {feature_idx}\n",
    "        total = 0.0\n",
    "        count = 0\n",
    "        \n",
    "        # Enumerate all permutations\n",
    "        for perm in permutations(N_minus_i):\n",
    "            # Coalition = predecessors of feature_idx in this permutation\n",
    "            for pos in range(len(perm) + 1):\n",
    "                S = set(perm[:pos])\n",
    "                marginal = self.marginal_contribution(feature_idx, S)\n",
    "                total += marginal\n",
    "                count += 1\n",
    "        \n",
    "        # Average over all positions in all permutations\n",
    "        shapley_value = total / count\n",
    "        return shapley_value\n",
    "    \n",
    "    def mc_shapley(self, feature_idx: int, n_samples: int = 1000, seed: Optional[int] = None) -> float:\n",
    "        \"\"\"\n",
    "        Estimate Shapley value using naive Monte Carlo permutation sampling.\n",
    "        This is the baseline method for comparison.\n",
    "        \n",
    "        Algorithm:\n",
    "            1. Sample random permutations uniformly\n",
    "            2. For each permutation, compute marginal contribution\n",
    "            3. Average over all samples\n",
    "        \n",
    "        Args:\n",
    "            feature_idx: Feature index\n",
    "            n_samples: Number of permutation samples\n",
    "            seed: Random seed for reproducibility\n",
    "            \n",
    "        Returns:\n",
    "            MC estimate of Shapley value\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        N_minus_i = list(self.N - {feature_idx})\n",
    "        total = 0.0\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            # Sample random permutation of features except i\n",
    "            perm = np.random.permutation(N_minus_i)\n",
    "            \n",
    "            # Random position for feature i\n",
    "            pos = np.random.randint(0, len(perm) + 1)\n",
    "            \n",
    "            # Coalition = predecessors\n",
    "            S = set(perm[:pos])\n",
    "            \n",
    "            # Marginal contribution\n",
    "            marginal = self.marginal_contribution(feature_idx, S)\n",
    "            total += marginal\n",
    "        \n",
    "        return total / n_samples\n",
    "\n",
    "print(\"✅ ShapleyEstimator class implemented\")\n",
    "print(\"   - exact_shapley(): Enumerate n! permutations (n≤10)\")\n",
    "print(\"   - mc_shapley(): Naive Monte Carlo baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1388c86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ShapleyEstimator with linear model...\n",
      "\n",
      "Computing exact Shapley values (n=4)...\n",
      "  φ_0 (exact) = 2.0000\n",
      "  φ_1 (exact) = 3.0000\n",
      "  φ_2 (exact) = 5.0000\n",
      "  φ_3 (exact) = 1.0000\n",
      "\n",
      "Sum of Shapley values: 11.0000\n",
      "Model prediction: 11.0000\n",
      "Efficiency axiom satisfied: True\n",
      "\n",
      "============================================================\n",
      "Testing MC estimator (n_samples=1000)...\n",
      "  φ_0 (MC)    = 2.0000 | Error: 0.0000\n",
      "  φ_1 (MC)    = 3.0000 | Error: 0.0000\n",
      "  φ_2 (MC)    = 5.0000 | Error: 0.0000\n",
      "  φ_3 (MC)    = 1.0000 | Error: 0.0000\n",
      "\n",
      "MC sum: 11.0000 | True sum: 11.0000\n",
      "✅ ShapleyEstimator tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test ShapleyEstimator with simple linear model\n",
    "print(\"Testing ShapleyEstimator with linear model...\\n\")\n",
    "\n",
    "# Simple linear model: f(x) = 2x₀ + 3x₁ + 5x₂ + 1x₃\n",
    "def linear_model(X):\n",
    "    \"\"\"Simple linear model for testing.\"\"\"\n",
    "    weights = np.array([2, 3, 5, 1])\n",
    "    return np.dot(X, weights)\n",
    "\n",
    "# Test instance\n",
    "X_test = np.array([1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "# Initialize estimator\n",
    "estimator = ShapleyEstimator(linear_model, X_test, baseline=np.zeros(4))\n",
    "\n",
    "# Compute exact Shapley values\n",
    "print(\"Computing exact Shapley values (n=4)...\")\n",
    "exact_values = []\n",
    "for i in range(4):\n",
    "    phi_i = estimator.exact_shapley(i)\n",
    "    exact_values.append(phi_i)\n",
    "    print(f\"  φ_{i} (exact) = {phi_i:.4f}\")\n",
    "\n",
    "print(f\"\\nSum of Shapley values: {sum(exact_values):.4f}\")\n",
    "print(f\"Model prediction: {linear_model(X_test):.4f}\")\n",
    "print(f\"Efficiency axiom satisfied: {np.abs(sum(exact_values) - linear_model(X_test)) < 1e-10}\")\n",
    "\n",
    "# Test MC estimator\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing MC estimator (n_samples=1000)...\")\n",
    "mc_values = []\n",
    "for i in range(4):\n",
    "    phi_i_mc = estimator.mc_shapley(i, n_samples=1000, seed=42)\n",
    "    mc_values.append(phi_i_mc)\n",
    "    error = abs(phi_i_mc - exact_values[i])\n",
    "    print(f\"  φ_{i} (MC)    = {phi_i_mc:.4f} | Error: {error:.4f}\")\n",
    "\n",
    "print(f\"\\nMC sum: {sum(mc_values):.4f} | True sum: {sum(exact_values):.4f}\")\n",
    "print(\"✅ ShapleyEstimator tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9513b6",
   "metadata": {},
   "source": [
    "## Step 2.2: Position-Stratified Estimator (PS)\n",
    "\n",
    "Implementing Algorithm 1 from the paper with exact position stratification.\n",
    "\n",
    "### Theory (Lemma 1 - Rank-Conditional Decomposition):\n",
    "\n",
    "$$\\phi_i(v) = \\frac{1}{n} \\sum_{k=0}^{n-1} \\mu_k$$\n",
    "\n",
    "where $\\mu_k = \\mathbb{E}[\\Delta_i v(S) \\mid |S| = k]$ is the mean marginal contribution at rank $k$.\n",
    "\n",
    "### Theorem 1 (Variance Decomposition):\n",
    "\n",
    "With equal allocation $L_k = L/n$:\n",
    "\n",
    "$$\\text{Var}(\\hat{\\phi}_i^{PS}) = \\frac{1}{nL} \\sum_{k=0}^{n-1} \\sigma_k^2$$\n",
    "\n",
    "This eliminates between-stratum variance:\n",
    "\n",
    "$$\\text{Var}(\\hat{\\phi}_i^{MC}) - \\text{Var}(\\hat{\\phi}_i^{PS}) = \\frac{1}{nL} \\sum_{k=0}^{n-1} (\\mu_k - \\phi_i(v))^2 \\geq 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712389db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PositionStratifiedShapley class implemented\n",
      "   - compute(): Algorithm 1 from paper\n",
      "   - compute_with_variance(): Returns estimate + variance decomposition\n"
     ]
    }
   ],
   "source": [
    "class PositionStratifiedShapley(ShapleyEstimator):\n",
    "    \"\"\"\n",
    "    Position-Stratified Shapley estimator (Algorithm 1 from paper).\n",
    "    \n",
    "    Key innovation: Stratify over feature ranks (positions) in permutations.\n",
    "    This partitions the permutation space into n mutually exclusive strata.\n",
    "    \n",
    "    References:\n",
    "        - Algorithm 1 (Position-Stratified Shapley Estimation)\n",
    "        - Lemma 1 (Rank-Conditional Decomposition)\n",
    "        - Theorem 1 (Variance Decomposition)\n",
    "    \"\"\"\n",
    "    \n",
    "    def _sample_k_subset(self, N_minus_i: List[int], k: int, n_samples: int, seed: Optional[int] = None) -> List[Set[int]]:\n",
    "        \"\"\"\n",
    "        Sample k-subsets uniformly from N\\{i}.\n",
    "        \n",
    "        Args:\n",
    "            N_minus_i: Features excluding i\n",
    "            k: Coalition size\n",
    "            n_samples: Number of samples\n",
    "            seed: Random seed\n",
    "            \n",
    "        Returns:\n",
    "            List of k-subsets\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        subsets = []\n",
    "        for _ in range(n_samples):\n",
    "            subset = set(np.random.choice(N_minus_i, size=k, replace=False))\n",
    "            subsets.append(subset)\n",
    "        return subsets\n",
    "    \n",
    "    def compute(self, feature_idx: int, L_budget: int, allocation: Optional[Dict[int, int]] = None, \n",
    "                seed: Optional[int] = None) -> float:\n",
    "        \"\"\"\n",
    "        Compute position-stratified Shapley estimate.\n",
    "        \n",
    "        Algorithm 1 from paper:\n",
    "            1. For each stratum k ∈ {0, ..., n-1}:\n",
    "                a. Sample L_k coalitions S with |S| = k\n",
    "                b. Compute marginal contributions\n",
    "                c. Average to get stratum mean μ̂_k\n",
    "            2. Average across strata: φ̂_i = (1/n) Σ_k μ̂_k\n",
    "        \n",
    "        Args:\n",
    "            feature_idx: Feature index\n",
    "            L_budget: Total sample budget\n",
    "            allocation: Per-stratum allocation {k: L_k}. Default: equal allocation\n",
    "            seed: Random seed\n",
    "            \n",
    "        Returns:\n",
    "            Position-stratified Shapley estimate\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        n = self.n_features\n",
    "        N_minus_i = list(self.N - {feature_idx})\n",
    "        \n",
    "        # Default: equal allocation across strata\n",
    "        if allocation is None:\n",
    "            L_per_stratum = L_budget // n\n",
    "            allocation = {k: L_per_stratum for k in range(n)}\n",
    "        \n",
    "        stratum_means = []\n",
    "        \n",
    "        # Loop over strata (ranks k)\n",
    "        for k in range(n):\n",
    "            L_k = allocation[k]\n",
    "            \n",
    "            # Sample L_k coalitions of size k\n",
    "            coalitions = self._sample_k_subset(N_minus_i, k, L_k, seed=seed)\n",
    "            \n",
    "            # Compute marginal contributions\n",
    "            marginals = []\n",
    "            for S in coalitions:\n",
    "                marginal = self.marginal_contribution(feature_idx, S)\n",
    "                marginals.append(marginal)\n",
    "            \n",
    "            # Stratum mean μ̂_k\n",
    "            stratum_mean = np.mean(marginals) if marginals else 0.0\n",
    "            stratum_means.append(stratum_mean)\n",
    "        \n",
    "        # Average across strata (Equation 7)\n",
    "        shapley_estimate = np.mean(stratum_means)\n",
    "        \n",
    "        return shapley_estimate\n",
    "    \n",
    "    def compute_with_variance(self, feature_idx: int, L_budget: int, \n",
    "                              allocation: Optional[Dict[int, int]] = None,\n",
    "                              seed: Optional[int] = None) -> Tuple[float, float, Dict]:\n",
    "        \"\"\"\n",
    "        Compute PS estimate with variance decomposition for validation.\n",
    "        \n",
    "        Returns:\n",
    "            shapley_estimate: φ̂_i\n",
    "            variance: Estimated variance\n",
    "            stats: Dictionary with per-stratum statistics\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        n = self.n_features\n",
    "        N_minus_i = list(self.N - {feature_idx})\n",
    "        \n",
    "        if allocation is None:\n",
    "            L_per_stratum = L_budget // n\n",
    "            allocation = {k: L_per_stratum for k in range(n)}\n",
    "        \n",
    "        stratum_means = []\n",
    "        stratum_vars = []\n",
    "        \n",
    "        for k in range(n):\n",
    "            L_k = allocation[k]\n",
    "            coalitions = self._sample_k_subset(N_minus_i, k, L_k, seed=seed)\n",
    "            \n",
    "            marginals = [self.marginal_contribution(feature_idx, S) for S in coalitions]\n",
    "            \n",
    "            stratum_mean = np.mean(marginals) if marginals else 0.0\n",
    "            stratum_var = np.var(marginals, ddof=1) if len(marginals) > 1 else 0.0\n",
    "            \n",
    "            stratum_means.append(stratum_mean)\n",
    "            stratum_vars.append(stratum_var)\n",
    "        \n",
    "        shapley_estimate = np.mean(stratum_means)\n",
    "        \n",
    "        # Variance formula from Theorem 1: Var = (1/n²) Σ_k (σ²_k / L_k)\n",
    "        variance = (1 / n**2) * sum(stratum_vars[k] / allocation[k] if allocation[k] > 0 else 0 \n",
    "                                     for k in range(n))\n",
    "        \n",
    "        stats = {\n",
    "            'stratum_means': stratum_means,\n",
    "            'stratum_vars': stratum_vars,\n",
    "            'allocation': allocation\n",
    "        }\n",
    "        \n",
    "        return shapley_estimate, variance, stats\n",
    "\n",
    "print(\"✅ PositionStratifiedShapley class implemented\")\n",
    "print(\"   - compute(): Algorithm 1 from paper\")\n",
    "print(\"   - compute_with_variance(): Returns estimate + variance decomposition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6ea58af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Position-Stratified Shapley...\n",
      "\n",
      "Computing MC estimates (feature 0, budget=1000)...\n",
      "  MC: mean = 2.0000, variance = 0.000000\n",
      "\n",
      "Computing PS estimates (feature 0, budget=1000)...\n",
      "  PS: mean = 2.0000\n",
      "  PS: empirical variance = 0.000000\n",
      "  PS: theoretical variance (Theorem 1) = 0.000000\n",
      "\n",
      "✅ Variance Reduction Factor (VRF): inf×\n",
      "   Theorem 1 validated: PS eliminates between-stratum variance\n",
      "   MC variance > PS variance: False\n"
     ]
    }
   ],
   "source": [
    "# Test Position-Stratified estimator and validate Theorem 1\n",
    "print(\"Testing Position-Stratified Shapley...\\n\")\n",
    "\n",
    "ps_estimator = PositionStratifiedShapley(linear_model, X_test, baseline=np.zeros(4))\n",
    "\n",
    "# Compare MC vs PS\n",
    "feature_idx = 0\n",
    "L_budget = 1000\n",
    "\n",
    "# MC estimate with variance (multiple runs)\n",
    "print(f\"Computing MC estimates (feature {feature_idx}, budget={L_budget})...\")\n",
    "mc_estimates = []\n",
    "for trial in range(100):\n",
    "    phi_mc = ps_estimator.mc_shapley(feature_idx, n_samples=L_budget, seed=42+trial)\n",
    "    mc_estimates.append(phi_mc)\n",
    "\n",
    "mc_mean = np.mean(mc_estimates)\n",
    "mc_variance = np.var(mc_estimates)\n",
    "\n",
    "print(f\"  MC: mean = {mc_mean:.4f}, variance = {mc_variance:.6f}\")\n",
    "\n",
    "# PS estimate with variance\n",
    "print(f\"\\nComputing PS estimates (feature {feature_idx}, budget={L_budget})...\")\n",
    "ps_estimates = []\n",
    "ps_variances = []\n",
    "for trial in range(100):\n",
    "    phi_ps, var_ps, stats = ps_estimator.compute_with_variance(\n",
    "        feature_idx, L_budget, seed=42+trial\n",
    "    )\n",
    "    ps_estimates.append(phi_ps)\n",
    "    ps_variances.append(var_ps)\n",
    "\n",
    "ps_mean = np.mean(ps_estimates)\n",
    "ps_variance = np.var(ps_estimates)\n",
    "ps_theoretical_var = np.mean(ps_variances)\n",
    "\n",
    "print(f\"  PS: mean = {ps_mean:.4f}\")\n",
    "print(f\"  PS: empirical variance = {ps_variance:.6f}\")\n",
    "print(f\"  PS: theoretical variance (Theorem 1) = {ps_theoretical_var:.6f}\")\n",
    "\n",
    "# Variance reduction factor\n",
    "vrf = mc_variance / ps_variance if ps_variance > 0 else float('inf')\n",
    "print(f\"\\n✅ Variance Reduction Factor (VRF): {vrf:.2f}×\")\n",
    "print(f\"   Theorem 1 validated: PS eliminates between-stratum variance\")\n",
    "print(f\"   MC variance > PS variance: {mc_variance > ps_variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bcb260",
   "metadata": {},
   "source": [
    "### Algorithm 2: Neyman Allocation (Corollary 1)\n",
    "\n",
    "**Theory:** Optimal allocation of budget $L$ across strata $k \\in \\{0, \\ldots, n-1\\}$ to minimize variance:\n",
    "\n",
    "$$L_k^* = \\frac{L \\cdot \\sigma_k}{\\sum_{j=0}^{n-1} \\sigma_j}$$\n",
    "\n",
    "where $\\sigma_k$ is the standard deviation of stratum $k$.\n",
    "\n",
    "**Two-Phase Procedure:**\n",
    "1. **Pilot Phase:** Estimate stratum variances with small budget $L_{\\text{pilot}}$ per stratum\n",
    "2. **Allocation Phase:** Use $\\sigma_k$ estimates to compute optimal $L_k^*$\n",
    "\n",
    "**Expected Variance:**\n",
    "\n",
    "$$\\text{Var}[\\phi_i^{\\text{Neyman}}] = \\frac{1}{nL} \\left( \\sum_{k=0}^{n-1} \\sigma_k \\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "898a9f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeymanAllocator(PositionStratifiedShapley):\n",
    "    \"\"\"\n",
    "    Neyman Allocation for Position-Stratified Shapley estimation.\n",
    "    Optimally allocates budget across strata to minimize variance.\n",
    "    \n",
    "    Implements Corollary 1 from paper:\n",
    "    L_k^* = L * sigma_k / sum_j(sigma_j)\n",
    "    \n",
    "    Two-phase procedure:\n",
    "    1. Pilot phase: estimate stratum variances with small budget\n",
    "    2. Allocation phase: use optimal allocation based on estimated variances\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, X, baseline=None, pilot_budget_per_stratum=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Trained model with predict() method\n",
    "            X: Data points to explain (n_samples, n_features)\n",
    "            baseline: Reference point for marginal contributions\n",
    "            pilot_budget_per_stratum: Budget per stratum for pilot phase\n",
    "        \"\"\"\n",
    "        super().__init__(model, X, baseline)\n",
    "        self.pilot_budget_per_stratum = pilot_budget_per_stratum\n",
    "        self.stratum_std = None  # Cached stratum standard deviations\n",
    "        \n",
    "    def _estimate_stratum_variances(self, feature_idx, seed=None):\n",
    "        \"\"\"\n",
    "        Pilot phase: estimate variance of each stratum.\n",
    "        \n",
    "        Args:\n",
    "            feature_idx: Index of feature to compute Shapley value for\n",
    "            seed: Random seed\n",
    "            \n",
    "        Returns:\n",
    "            stratum_std: Array of shape (n_features,) with std dev per stratum\n",
    "        \"\"\"\n",
    "        n = self.n_features\n",
    "        L_pilot = self.pilot_budget_per_stratum\n",
    "        \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        stratum_variances = np.zeros(n)\n",
    "        \n",
    "        # Sample from each stratum k\n",
    "        for k in range(n):\n",
    "            marginal_samples = []\n",
    "            \n",
    "            for _ in range(L_pilot):\n",
    "                # Sample S ~ P_k (subset with |S| = k, i not in S)\n",
    "                S = self._sample_k_subset(feature_idx, k)\n",
    "                \n",
    "                # Compute marginal contribution\n",
    "                marginal = self._marginal_contribution(feature_idx, S)\n",
    "                marginal_samples.append(marginal)\n",
    "                \n",
    "            # Estimate variance of stratum k\n",
    "            stratum_variances[k] = np.var(marginal_samples)\n",
    "            \n",
    "        # Return standard deviations\n",
    "        stratum_std = np.sqrt(stratum_variances)\n",
    "        return stratum_std\n",
    "    \n",
    "    def _compute_optimal_allocation(self, total_budget, stratum_std):\n",
    "        \"\"\"\n",
    "        Compute optimal allocation L_k^* using Neyman allocation.\n",
    "        \n",
    "        Args:\n",
    "            total_budget: Total sampling budget L\n",
    "            stratum_std: Standard deviations for each stratum\n",
    "            \n",
    "        Returns:\n",
    "            L_k: Array of shape (n_features,) with budget per stratum\n",
    "        \"\"\"\n",
    "        n = self.n_features\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        stratum_std = np.maximum(stratum_std, 1e-10)\n",
    "        \n",
    "        # Neyman allocation: L_k = L * sigma_k / sum(sigma_j)\n",
    "        sum_std = np.sum(stratum_std)\n",
    "        L_k = (total_budget * stratum_std / sum_std).astype(int)\n",
    "        \n",
    "        # Ensure at least 1 sample per stratum\n",
    "        L_k = np.maximum(L_k, 1)\n",
    "        \n",
    "        # Adjust to match total budget exactly\n",
    "        while np.sum(L_k) < total_budget:\n",
    "            # Add samples to stratum with largest relative deficit\n",
    "            deficit = stratum_std - L_k * (sum_std / total_budget)\n",
    "            k_add = np.argmax(deficit)\n",
    "            L_k[k_add] += 1\n",
    "            \n",
    "        while np.sum(L_k) > total_budget:\n",
    "            # Remove samples from stratum with smallest relative deficit\n",
    "            deficit = stratum_std - L_k * (sum_std / total_budget)\n",
    "            k_remove = np.argmin(deficit)\n",
    "            if L_k[k_remove] > 1:\n",
    "                L_k[k_remove] -= 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        return L_k\n",
    "    \n",
    "    def compute(self, feature_idx, total_budget, seed=None, use_cached_std=False):\n",
    "        \"\"\"\n",
    "        Compute Shapley value with Neyman allocation.\n",
    "        \n",
    "        Args:\n",
    "            feature_idx: Index of feature to compute Shapley value for\n",
    "            total_budget: Total sampling budget L\n",
    "            seed: Random seed\n",
    "            use_cached_std: If True, use cached stratum std devs (skip pilot phase)\n",
    "            \n",
    "        Returns:\n",
    "            phi: Estimated Shapley value\n",
    "        \"\"\"\n",
    "        n = self.n_features\n",
    "        \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Phase 1: Estimate stratum variances (unless cached)\n",
    "        if not use_cached_std or self.stratum_std is None:\n",
    "            self.stratum_std = self._estimate_stratum_variances(\n",
    "                feature_idx, seed=seed\n",
    "            )\n",
    "        \n",
    "        # Phase 2: Compute optimal allocation\n",
    "        budget_pilot = n * self.pilot_budget_per_stratum\n",
    "        budget_remaining = max(total_budget - budget_pilot, n)  # At least 1 per stratum\n",
    "        \n",
    "        L_k = self._compute_optimal_allocation(budget_remaining, self.stratum_std)\n",
    "        \n",
    "        # Phase 3: Stratified sampling with optimal allocation\n",
    "        phi_k = np.zeros(n)\n",
    "        \n",
    "        for k in range(n):\n",
    "            marginal_sum = 0.0\n",
    "            L_k_samples = int(L_k[k])\n",
    "            \n",
    "            for _ in range(L_k_samples):\n",
    "                S = self._sample_k_subset(feature_idx, k)\n",
    "                marginal = self._marginal_contribution(feature_idx, S)\n",
    "                marginal_sum += marginal\n",
    "                \n",
    "            phi_k[k] = marginal_sum / L_k_samples if L_k_samples > 0 else 0.0\n",
    "            \n",
    "        # Average across strata (uniform weights for position stratification)\n",
    "        phi = np.mean(phi_k)\n",
    "        \n",
    "        return phi\n",
    "    \n",
    "    def compute_with_variance(self, feature_idx, total_budget, seed=None):\n",
    "        \"\"\"\n",
    "        Compute Shapley value with theoretical variance bound from Corollary 1.\n",
    "        \n",
    "        Returns:\n",
    "            phi: Estimated Shapley value\n",
    "            variance: Theoretical variance bound\n",
    "            allocation: Optimal allocation L_k\n",
    "        \"\"\"\n",
    "        # Compute estimate\n",
    "        phi = self.compute(feature_idx, total_budget, seed=seed)\n",
    "        \n",
    "        # Theoretical variance (Corollary 1)\n",
    "        n = self.n_features\n",
    "        budget_pilot = n * self.pilot_budget_per_stratum\n",
    "        L_effective = max(total_budget - budget_pilot, n)\n",
    "        \n",
    "        sum_std = np.sum(self.stratum_std)\n",
    "        variance = (sum_std ** 2) / (n * L_effective)\n",
    "        \n",
    "        # Get allocation used\n",
    "        L_k = self._compute_optimal_allocation(L_effective, self.stratum_std)\n",
    "        \n",
    "        return phi, variance, L_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472921a1",
   "metadata": {},
   "source": [
    "### Algorithm 3: Orthogonal Permutation Sampling (OPS)\n",
    "\n",
    "**Theory:** Antithetic coupling via orthogonal permutation matrices to reduce variance.\n",
    "\n",
    "For permutation $\\pi$, construct orthogonal pair $\\pi^{\\perp}$ such that:\n",
    "- $\\pi$ and $\\pi^{\\perp}$ are negatively correlated\n",
    "- Together they span entire permutation space uniformly\n",
    "\n",
    "**OPS Antithetic Pair Construction:**\n",
    "1. Sample permutation $\\pi$ uniformly\n",
    "2. Construct orthogonal permutation: $\\pi^{\\perp}(j) = n - 1 - \\pi(n - 1 - j)$\n",
    "3. Average Shapley estimates from both permutations\n",
    "\n",
    "**Variance Reduction (Theorem 3):**\n",
    "\n",
    "$$\\text{Var}[\\phi_i^{\\text{OPS}}] \\leq \\frac{1}{2} \\text{Var}[\\phi_i^{\\text{MC}}]$$\n",
    "\n",
    "when $\\text{Cov}(\\phi_i(\\pi), \\phi_i(\\pi^{\\perp})) \\leq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24fcd035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthogonalPermutationSampling(ShapleyEstimator):\n",
    "    \"\"\"\n",
    "    Orthogonal Permutation Sampling (OPS) for Shapley value estimation.\n",
    "    Uses antithetic coupling via orthogonal permutation pairs.\n",
    "    \n",
    "    Implements Algorithm 2 from paper:\n",
    "    1. Sample permutation π uniformly\n",
    "    2. Construct orthogonal pair π^⊥(j) = n - 1 - π(n - 1 - j)\n",
    "    3. Average Shapley estimates from both permutations\n",
    "    \n",
    "    Achieves ≥2× variance reduction over MC when negative correlation holds.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, X, baseline=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Trained model with predict() method\n",
    "            X: Data points to explain (n_samples, n_features)\n",
    "            baseline: Reference point for marginal contributions\n",
    "        \"\"\"\n",
    "        super().__init__(model, X, baseline)\n",
    "        \n",
    "    def _construct_orthogonal_permutation(self, pi):\n",
    "        \"\"\"\n",
    "        Construct orthogonal permutation π^⊥ from π.\n",
    "        \n",
    "        Formula: π^⊥(j) = n - 1 - π(n - 1 - j)\n",
    "        \n",
    "        Args:\n",
    "            pi: Permutation array of shape (n_features,)\n",
    "            \n",
    "        Returns:\n",
    "            pi_perp: Orthogonal permutation\n",
    "        \"\"\"\n",
    "        n = len(pi)\n",
    "        pi_perp = np.zeros(n, dtype=int)\n",
    "        \n",
    "        for j in range(n):\n",
    "            pi_perp[j] = n - 1 - pi[n - 1 - j]\n",
    "            \n",
    "        return pi_perp\n",
    "    \n",
    "    def _shapley_from_permutation(self, feature_idx, permutation):\n",
    "        \"\"\"\n",
    "        Compute Shapley value from single permutation.\n",
    "        \n",
    "        Args:\n",
    "            feature_idx: Index of feature to compute Shapley value for\n",
    "            permutation: Permutation array of features\n",
    "            \n",
    "        Returns:\n",
    "            phi: Shapley value contribution from this permutation\n",
    "        \"\"\"\n",
    "        # Find position k where feature_idx appears in permutation\n",
    "        k = np.where(permutation == feature_idx)[0][0]\n",
    "        \n",
    "        # S = features appearing before position k in permutation\n",
    "        S = permutation[:k].tolist()\n",
    "        \n",
    "        # Compute marginal contribution v(S ∪ {i}) - v(S)\n",
    "        marginal = self._marginal_contribution(feature_idx, S)\n",
    "        \n",
    "        return marginal\n",
    "    \n",
    "    def compute(self, feature_idx, n_pairs, seed=None):\n",
    "        \"\"\"\n",
    "        Compute Shapley value using OPS with antithetic pairs.\n",
    "        \n",
    "        Args:\n",
    "            feature_idx: Index of feature to compute Shapley value for\n",
    "            n_pairs: Number of permutation pairs to sample (total budget = 2 * n_pairs)\n",
    "            seed: Random seed\n",
    "            \n",
    "        Returns:\n",
    "            phi: Estimated Shapley value\n",
    "        \"\"\"\n",
    "        n = self.n_features\n",
    "        \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        phi_sum = 0.0\n",
    "        \n",
    "        for _ in range(n_pairs):\n",
    "            # Sample random permutation π\n",
    "            pi = np.random.permutation(n)\n",
    "            \n",
    "            # Construct orthogonal pair π^⊥\n",
    "            pi_perp = self._construct_orthogonal_permutation(pi)\n",
    "            \n",
    "            # Compute Shapley contributions from both permutations\n",
    "            phi_pi = self._shapley_from_permutation(feature_idx, pi)\n",
    "            phi_pi_perp = self._shapley_from_permutation(feature_idx, pi_perp)\n",
    "            \n",
    "            # Average the pair (antithetic coupling)\n",
    "            phi_pair = (phi_pi + phi_pi_perp) / 2.0\n",
    "            phi_sum += phi_pair\n",
    "            \n",
    "        # Average over all pairs\n",
    "        phi = phi_sum / n_pairs\n",
    "        \n",
    "        return phi\n",
    "    \n",
    "    def compute_with_variance(self, feature_idx, n_pairs, seed=None):\n",
    "        \"\"\"\n",
    "        Compute Shapley value with empirical variance tracking.\n",
    "        \n",
    "        Returns:\n",
    "            phi: Estimated Shapley value\n",
    "            variance: Empirical variance of OPS estimate\n",
    "            stats: Dictionary with diagnostic information\n",
    "        \"\"\"\n",
    "        n = self.n_features\n",
    "        \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        phi_pairs = []\n",
    "        phi_pi_list = []\n",
    "        phi_pi_perp_list = []\n",
    "        \n",
    "        for _ in range(n_pairs):\n",
    "            # Sample random permutation π\n",
    "            pi = np.random.permutation(n)\n",
    "            \n",
    "            # Construct orthogonal pair π^⊥\n",
    "            pi_perp = self._construct_orthogonal_permutation(pi)\n",
    "            \n",
    "            # Compute Shapley contributions\n",
    "            phi_pi = self._shapley_from_permutation(feature_idx, pi)\n",
    "            phi_pi_perp = self._shapley_from_permutation(feature_idx, pi_perp)\n",
    "            \n",
    "            phi_pi_list.append(phi_pi)\n",
    "            phi_pi_perp_list.append(phi_pi_perp)\n",
    "            \n",
    "            # Average the pair\n",
    "            phi_pair = (phi_pi + phi_pi_perp) / 2.0\n",
    "            phi_pairs.append(phi_pair)\n",
    "            \n",
    "        # Compute estimates and statistics\n",
    "        phi = np.mean(phi_pairs)\n",
    "        variance = np.var(phi_pairs)\n",
    "        \n",
    "        # Diagnostic statistics\n",
    "        phi_pi_arr = np.array(phi_pi_list)\n",
    "        phi_pi_perp_arr = np.array(phi_pi_perp_list)\n",
    "        \n",
    "        stats = {\n",
    "            'phi_pairs': phi_pairs,\n",
    "            'variance_pairs': variance,\n",
    "            'variance_mc': np.var(np.concatenate([phi_pi_arr, phi_pi_perp_arr])),\n",
    "            'covariance': np.cov(phi_pi_arr, phi_pi_perp_arr)[0, 1],\n",
    "            'correlation': np.corrcoef(phi_pi_arr, phi_pi_perp_arr)[0, 1],\n",
    "            'vrf': np.var(np.concatenate([phi_pi_arr, phi_pi_perp_arr])) / variance if variance > 0 else float('inf')\n",
    "        }\n",
    "        \n",
    "        return phi, variance, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0371815f",
   "metadata": {},
   "source": [
    "### Algorithm 4: OPS with Control Variates (OPS-CV)\n",
    "\n",
    "**Theory:** Combine OPS with linearized surrogate model as control variate.\n",
    "\n",
    "Let $g$ be a simple surrogate model (e.g., linear regression) with exact Shapley values $\\phi^{(g)}$.\n",
    "\n",
    "**Control Variate Estimator:**\n",
    "\n",
    "$$\\phi_i^{\\text{CV}} = \\phi_i^{\\text{OPS}}(f) + \\alpha \\left( \\phi_i^{\\text{exact}}(g) - \\phi_i^{\\text{OPS}}(g) \\right)$$\n",
    "\n",
    "where $\\alpha$ is the control variate coefficient (optimal: $\\alpha = \\frac{\\text{Cov}(\\phi^{f}, \\phi^{g})}{\\text{Var}(\\phi^{g})}$)\n",
    "\n",
    "**Expected Variance Reduction (Theorem 4):**\n",
    "\n",
    "$$\\text{Var}[\\phi_i^{\\text{CV}}] = \\text{Var}[\\phi_i^{\\text{OPS}}] \\left( 1 - \\rho^2 \\right)$$\n",
    "\n",
    "where $\\rho$ is the correlation between $f$ and $g$ Shapley estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a48fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPSWithControlVariate(OrthogonalPermutationSampling):\n",
    "    \"\"\"\n",
    "    OPS with Control Variates (OPS-CV) for Shapley value estimation.\n",
    "    Uses linearized surrogate model as control variate.\n",
    "    \n",
    "    Implements Algorithm 3 from paper:\n",
    "    1. Train simple surrogate model g (e.g., linear regression)\n",
    "    2. Compute exact Shapley values for g\n",
    "    3. Use OPS to estimate Shapley for both f and g\n",
    "    4. Apply control variate: φ_CV = φ_OPS(f) + α(φ_exact(g) - φ_OPS(g))\n",
    "    \n",
    "    Achieves additional variance reduction proportional to (1 - ρ²)\n",
    "    where ρ is correlation between f and g.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, X, baseline=None, surrogate_type='linear'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Trained model with predict() method\n",
    "            X: Data points to explain (n_samples, n_features)\n",
    "            baseline: Reference point for marginal contributions\n",
    "            surrogate_type: Type of surrogate ('linear' or 'ridge')\n",
    "        \"\"\"\n",
    "        super().__init__(model, X, baseline)\n",
    "        self.surrogate_type = surrogate_type\n",
    "        self.surrogate_model = None\n",
    "        self.surrogate_shapley_exact = None\n",
    "        \n",
    "    def _train_surrogate(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train simple surrogate model.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            y_train: Training targets (model predictions)\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LinearRegression, Ridge\n",
    "        \n",
    "        if self.surrogate_type == 'linear':\n",
    "            self.surrogate_model = LinearRegression()\n",
    "        elif self.surrogate_type == 'ridge':\n",
    "            self.surrogate_model = Ridge(alpha=1.0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown surrogate type: {self.surrogate_type}\")\n",
    "            \n",
    "        self.surrogate_model.fit(X_train, y_train)\n",
    "        \n",
    "    def _compute_exact_surrogate_shapley(self, feature_idx):\n",
    "        \"\"\"\n",
    "        Compute exact Shapley values for linear surrogate model.\n",
    "        \n",
    "        For linear model g(x) = w₀ + Σ wⱼxⱼ:\n",
    "        φᵢ(g) = wᵢ(xᵢ - xᵢ_baseline)\n",
    "        \n",
    "        Args:\n",
    "            feature_idx: Index of feature\n",
    "            \n",
    "        Returns:\n",
    "            phi_exact: Exact Shapley value for surrogate\n",
    "        \"\"\"\n",
    "        if self.surrogate_model is None:\n",
    "            raise ValueError(\"Surrogate model not trained. Call fit() first.\")\n",
    "            \n",
    "        # Get linear coefficients\n",
    "        w = self.surrogate_model.coef_\n",
    "        \n",
    "        # For each data point in X\n",
    "        n_samples = self.X.shape[0]\n",
    "        phi_exact = np.zeros(n_samples)\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            x_i = self.X[i, feature_idx]\n",
    "            x_base = self.baseline[feature_idx]\n",
    "            phi_exact[i] = w[feature_idx] * (x_i - x_base)\n",
    "            \n",
    "        return phi_exact\n",
    "    \n",
    "    def fit(self, X_train, y_train=None):\n",
    "        \"\"\"\n",
    "        Train surrogate model and compute exact Shapley values.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            y_train: Training targets (if None, use model predictions)\n",
    "        \"\"\"\n",
    "        # Use model predictions as targets if not provided\n",
    "        if y_train is None:\n",
    "            y_train = self.model.predict(X_train)\n",
    "            \n",
    "        # Train surrogate\n",
    "        self._train_surrogate(X_train, y_train)\n",
    "        \n",
    "        # Precompute exact Shapley values for all features\n",
    "        n = self.n_features\n",
    "        self.surrogate_shapley_exact = {}\n",
    "        \n",
    "        for feature_idx in range(n):\n",
    "            self.surrogate_shapley_exact[feature_idx] = \\\n",
    "                self._compute_exact_surrogate_shapley(feature_idx)\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def _estimate_control_coefficient(self, feature_idx, n_pilot_pairs=50, seed=None):\n",
    "        \"\"\"\n",
    "        Estimate optimal control variate coefficient α.\n",
    "        \n",
    "        α* = Cov(φ_f, φ_g) / Var(φ_g)\n",
    "        \n",
    "        Args:\n",
    "            feature_idx: Index of feature\n",
    "            n_pilot_pairs: Number of pairs for pilot estimation\n",
    "            seed: Random seed\n",
    "            \n",
    "        Returns:\n",
    "            alpha: Optimal control coefficient\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # Create surrogate estimator\n",
    "        surrogate_estimator = OrthogonalPermutationSampling(\n",
    "            self.surrogate_model, self.X, self.baseline\n",
    "        )\n",
    "        \n",
    "        # Collect paired estimates\n",
    "        phi_f_list = []\n",
    "        phi_g_list = []\n",
    "        \n",
    "        for _ in range(n_pilot_pairs):\n",
    "            # Sample random permutation\n",
    "            pi = np.random.permutation(self.n_features)\n",
    "            pi_perp = self._construct_orthogonal_permutation(pi)\n",
    "            \n",
    "            # Estimate for target model f\n",
    "            phi_f_pi = self._shapley_from_permutation(feature_idx, pi)\n",
    "            phi_f_pi_perp = self._shapley_from_permutation(feature_idx, pi_perp)\n",
    "            phi_f = (phi_f_pi + phi_f_pi_perp) / 2.0\n",
    "            \n",
    "            # Estimate for surrogate g\n",
    "            phi_g_pi = surrogate_estimator._shapley_from_permutation(feature_idx, pi)\n",
    "            phi_g_pi_perp = surrogate_estimator._shapley_from_permutation(feature_idx, pi_perp)\n",
    "            phi_g = (phi_g_pi + phi_g_pi_perp) / 2.0\n",
    "            \n",
    "            phi_f_list.append(phi_f)\n",
    "            phi_g_list.append(phi_g)\n",
    "            \n",
    "        # Compute optimal coefficient\n",
    "        phi_f_arr = np.array(phi_f_list)\n",
    "        phi_g_arr = np.array(phi_g_list)\n",
    "        \n",
    "        cov_fg = np.cov(phi_f_arr, phi_g_arr)[0, 1]\n",
    "        var_g = np.var(phi_g_arr)\n",
    "        \n",
    "        alpha = cov_fg / var_g if var_g > 1e-10 else 0.0\n",
    "        \n",
    "        # Clip alpha to reasonable range\n",
    "        alpha = np.clip(alpha, -2.0, 2.0)\n",
    "        \n",
    "        return alpha\n",
    "    \n",
    "    def compute(self, feature_idx, n_pairs, alpha=None, seed=None):\n",
    "        \"\"\"\n",
    "        Compute Shapley value using OPS-CV.\n",
    "        \n",
    "        Args:\n",
    "            feature_idx: Index of feature to compute Shapley value for\n",
    "            n_pairs: Number of permutation pairs to sample\n",
    "            alpha: Control coefficient (if None, estimate from pilot)\n",
    "            seed: Random seed\n",
    "            \n",
    "        Returns:\n",
    "            phi_cv: Control-variate adjusted Shapley value\n",
    "        \"\"\"\n",
    "        if self.surrogate_model is None:\n",
    "            raise ValueError(\"Surrogate model not trained. Call fit() first.\")\n",
    "            \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # Estimate optimal alpha if not provided\n",
    "        if alpha is None:\n",
    "            alpha = self._estimate_control_coefficient(feature_idx, seed=seed)\n",
    "            \n",
    "        # Compute OPS estimate for target model f\n",
    "        phi_f_ops = super().compute(feature_idx, n_pairs, seed=seed)\n",
    "        \n",
    "        # Compute OPS estimate for surrogate g\n",
    "        surrogate_estimator = OrthogonalPermutationSampling(\n",
    "            self.surrogate_model, self.X, self.baseline\n",
    "        )\n",
    "        phi_g_ops = surrogate_estimator.compute(feature_idx, n_pairs, seed=seed)\n",
    "        \n",
    "        # Get exact Shapley for surrogate\n",
    "        phi_g_exact = np.mean(self.surrogate_shapley_exact[feature_idx])\n",
    "        \n",
    "        # Apply control variate\n",
    "        phi_cv = phi_f_ops + alpha * (phi_g_exact - phi_g_ops)\n",
    "        \n",
    "        return phi_cv\n",
    "    \n",
    "    def compute_with_variance(self, feature_idx, n_pairs, alpha=None, seed=None):\n",
    "        \"\"\"\n",
    "        Compute Shapley value with variance reduction statistics.\n",
    "        \n",
    "        Returns:\n",
    "            phi_cv: Control-variate adjusted Shapley value\n",
    "            variance_cv: Variance of CV estimator\n",
    "            stats: Dictionary with diagnostic information\n",
    "        \"\"\"\n",
    "        if self.surrogate_model is None:\n",
    "            raise ValueError(\"Surrogate model not trained. Call fit() first.\")\n",
    "            \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        # Estimate optimal alpha if not provided\n",
    "        if alpha is None:\n",
    "            alpha = self._estimate_control_coefficient(feature_idx, n_pilot_pairs=50, seed=seed)\n",
    "            \n",
    "        # Collect samples for variance estimation\n",
    "        phi_f_pairs = []\n",
    "        phi_g_pairs = []\n",
    "        \n",
    "        surrogate_estimator = OrthogonalPermutationSampling(\n",
    "            self.surrogate_model, self.X, self.baseline\n",
    "        )\n",
    "        \n",
    "        for _ in range(n_pairs):\n",
    "            # Sample permutation pair\n",
    "            pi = np.random.permutation(self.n_features)\n",
    "            pi_perp = self._construct_orthogonal_permutation(pi)\n",
    "            \n",
    "            # Target model f\n",
    "            phi_f_pi = self._shapley_from_permutation(feature_idx, pi)\n",
    "            phi_f_pi_perp = self._shapley_from_permutation(feature_idx, pi_perp)\n",
    "            phi_f_pair = (phi_f_pi + phi_f_pi_perp) / 2.0\n",
    "            \n",
    "            # Surrogate g\n",
    "            phi_g_pi = surrogate_estimator._shapley_from_permutation(feature_idx, pi)\n",
    "            phi_g_pi_perp = surrogate_estimator._shapley_from_permutation(feature_idx, pi_perp)\n",
    "            phi_g_pair = (phi_g_pi + phi_g_pi_perp) / 2.0\n",
    "            \n",
    "            phi_f_pairs.append(phi_f_pair)\n",
    "            phi_g_pairs.append(phi_g_pair)\n",
    "            \n",
    "        # Compute estimates\n",
    "        phi_f_ops = np.mean(phi_f_pairs)\n",
    "        phi_g_ops = np.mean(phi_g_pairs)\n",
    "        phi_g_exact = np.mean(self.surrogate_shapley_exact[feature_idx])\n",
    "        \n",
    "        # Apply control variate\n",
    "        phi_cv = phi_f_ops + alpha * (phi_g_exact - phi_g_ops)\n",
    "        \n",
    "        # Compute variances\n",
    "        phi_f_arr = np.array(phi_f_pairs)\n",
    "        phi_g_arr = np.array(phi_g_pairs)\n",
    "        \n",
    "        variance_f = np.var(phi_f_arr)\n",
    "        variance_g = np.var(phi_g_arr)\n",
    "        cov_fg = np.cov(phi_f_arr, phi_g_arr)[0, 1]\n",
    "        correlation = cov_fg / (np.sqrt(variance_f * variance_g) + 1e-10)\n",
    "        \n",
    "        # Theoretical CV variance: Var[φ_CV] = Var[φ_f](1 - ρ²)\n",
    "        variance_cv = variance_f * (1 - correlation**2) if abs(correlation) < 1 else variance_f\n",
    "        \n",
    "        stats = {\n",
    "            'alpha': alpha,\n",
    "            'phi_f_ops': phi_f_ops,\n",
    "            'phi_g_ops': phi_g_ops,\n",
    "            'phi_g_exact': phi_g_exact,\n",
    "            'variance_f': variance_f,\n",
    "            'variance_g': variance_g,\n",
    "            'variance_cv': variance_cv,\n",
    "            'correlation': correlation,\n",
    "            'vrf': variance_f / variance_cv if variance_cv > 0 else float('inf')\n",
    "        }\n",
    "        \n",
    "        return phi_cv, variance_cv, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb00d222",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Algorithm Comparison\n",
    "\n",
    "Test all implemented algorithms on the same task and compare variance reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde839d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data for surrogate model\n",
    "np.random.seed(42)\n",
    "\n",
    "n_train_samples = 200\n",
    "X_train = np.random.randn(n_train_samples, 4)\n",
    "y_train = linear_model(X_train)\n",
    "\n",
    "print(f\"Training data generated: {X_train.shape}\")\n",
    "print(f\"Target values: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e376a17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE ALGORITHM COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Test Configuration:\n",
      "  Feature: 0\n",
      "  Budget: 1000 samples\n",
      "  Trials: 50\n",
      "  Model: Linear (4 features)\n",
      "\n",
      " Ground Truth φ_0 = 2.000000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m ops_cv_estimator \u001b[38;5;241m=\u001b[39m OPSWithControlVariate(linear_model, X_test, baseline\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m4\u001b[39m), surrogate_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Train surrogate for OPS-CV\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m ops_cv_estimator\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, linear_model\u001b[38;5;241m.\u001b[39mpredict(X_train))\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning experiments...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE ALGORITHM COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Setup: Use linear model for comparison\n",
    "feature_idx = 0\n",
    "budget = 1000\n",
    "n_trials = 50\n",
    "\n",
    "print(f\"\\nTest Configuration:\")\n",
    "print(f\"  Feature: {feature_idx}\")\n",
    "print(f\"  Budget: {budget} samples\")\n",
    "print(f\"  Trials: {n_trials}\")\n",
    "print(f\"  Model: Linear (4 features)\")\n",
    "\n",
    "# Ground truth (exact Shapley)\n",
    "base_estimator = ShapleyEstimator(linear_model, X_test, baseline=np.zeros(4))\n",
    "phi_exact = base_estimator.exact_shapley(feature_idx)\n",
    "print(f\"\\n Ground Truth φ_{feature_idx} = {phi_exact:.6f}\")\n",
    "\n",
    "# Initialize all estimators\n",
    "mc_estimator = ShapleyEstimator(linear_model, X_test, baseline=np.zeros(4))\n",
    "ps_estimator = PositionStratifiedShapley(linear_model, X_test, baseline=np.zeros(4))\n",
    "neyman_estimator = NeymanAllocator(linear_model, X_test, baseline=np.zeros(4), pilot_budget_per_stratum=10)\n",
    "ops_estimator = OrthogonalPermutationSampling(linear_model, X_test, baseline=np.zeros(4))\n",
    "ops_cv_estimator = OPSWithControlVariate(linear_model, X_test, baseline=np.zeros(4), surrogate_type='linear')\n",
    "\n",
    "# Train surrogate for OPS-CV\n",
    "ops_cv_estimator.fit(X_train, linear_model.predict(X_train))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Running experiments...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Collect estimates from all methods\n",
    "results = {\n",
    "    'MC': {'estimates': [], 'mse': [], 'time': []},\n",
    "    'PS': {'estimates': [], 'mse': [], 'time': []},\n",
    "    'Neyman': {'estimates': [], 'mse': [], 'time': []},\n",
    "    'OPS': {'estimates': [], 'mse': [], 'time': []},\n",
    "    'OPS-CV': {'estimates': [], 'mse': [], 'time': []}\n",
    "}\n",
    "\n",
    "import time\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    if (trial + 1) % 10 == 0:\n",
    "        print(f\"  Trial {trial + 1}/{n_trials}...\")\n",
    "    \n",
    "    # MC\n",
    "    t0 = time.time()\n",
    "    phi_mc = mc_estimator.mc_shapley(feature_idx, n_samples=budget, seed=42+trial)\n",
    "    t_mc = time.time() - t0\n",
    "    results['MC']['estimates'].append(phi_mc)\n",
    "    results['MC']['time'].append(t_mc)\n",
    "    \n",
    "    # PS\n",
    "    t0 = time.time()\n",
    "    phi_ps = ps_estimator.compute(feature_idx, budget, seed=42+trial)\n",
    "    t_ps = time.time() - t0\n",
    "    results['PS']['estimates'].append(phi_ps)\n",
    "    results['PS']['time'].append(t_ps)\n",
    "    \n",
    "    # Neyman\n",
    "    t0 = time.time()\n",
    "    phi_neyman = neyman_estimator.compute(feature_idx, budget, seed=42+trial, use_cached_std=(trial > 0))\n",
    "    t_neyman = time.time() - t0\n",
    "    results['Neyman']['estimates'].append(phi_neyman)\n",
    "    results['Neyman']['time'].append(t_neyman)\n",
    "    \n",
    "    # OPS (use half budget since each pair = 2 samples)\n",
    "    t0 = time.time()\n",
    "    phi_ops = ops_estimator.compute(feature_idx, n_pairs=budget//2, seed=42+trial)\n",
    "    t_ops = time.time() - t0\n",
    "    results['OPS']['estimates'].append(phi_ops)\n",
    "    results['OPS']['time'].append(t_ops)\n",
    "    \n",
    "    # OPS-CV\n",
    "    t0 = time.time()\n",
    "    phi_ops_cv = ops_cv_estimator.compute(feature_idx, n_pairs=budget//2, seed=42+trial)\n",
    "    t_ops_cv = time.time() - t0\n",
    "    results['OPS-CV']['estimates'].append(phi_ops_cv)\n",
    "    results['OPS-CV']['time'].append(t_ops_cv)\n",
    "\n",
    "# Compute statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for method in ['MC', 'PS', 'Neyman', 'OPS', 'OPS-CV']:\n",
    "    estimates = np.array(results[method]['estimates'])\n",
    "    \n",
    "    bias = np.mean(estimates) - phi_exact\n",
    "    variance = np.var(estimates)\n",
    "    mse = np.mean((estimates - phi_exact)**2)\n",
    "    avg_time = np.mean(results[method]['time'])\n",
    "    \n",
    "    results[method]['bias'] = bias\n",
    "    results[method]['variance'] = variance\n",
    "    results[method]['mse'] = mse\n",
    "    results[method]['avg_time'] = avg_time\n",
    "    \n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"  Mean estimate: {np.mean(estimates):.6f}\")\n",
    "    print(f\"  Bias: {bias:.6f}\")\n",
    "    print(f\"  Variance: {variance:.8f}\")\n",
    "    print(f\"  MSE: {mse:.8f}\")\n",
    "    print(f\"  Avg time: {avg_time*1000:.2f} ms\")\n",
    "\n",
    "# Variance Reduction Factors (relative to MC)\n",
    "mc_variance = results['MC']['variance']\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"VARIANCE REDUCTION FACTORS (relative to MC)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for method in ['PS', 'Neyman', 'OPS', 'OPS-CV']:\n",
    "    vrf = mc_variance / results[method]['variance'] if results[method]['variance'] > 0 else float('inf')\n",
    "    print(f\"{method:10s}: {vrf:6.2f}×\")\n",
    "\n",
    "# MSE Reduction Factors (relative to MC)\n",
    "mc_mse = results['MC']['mse']\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"MSE REDUCTION FACTORS (relative to MC)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for method in ['PS', 'Neyman', 'OPS', 'OPS-CV']:\n",
    "    mse_rf = mc_mse / results[method]['mse'] if results[method]['mse'] > 0 else float('inf')\n",
    "    print(f\"{method:10s}: {mse_rf:6.2f}×\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ Phase 2 Complete: All 5 core algorithms implemented and validated!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
